# -*- coding: utf-8 -*-
"""CODE_ANALYSIS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EvQRxa5yFOXVqPevffIekQN4FZkS9GFT

# üìä Projeto de Compara√ß√£o de Modelos de Regress√£o com Multicolinearidade

## üéØ Objetivo
Avaliar o desempenho de diferentes t√©cnicas de regress√£o aplicadas a um conjunto de dados que apresenta **multicolinearidade**, com o intuito de identificar o modelo que melhor se ajusta aos dados.

## üß© Etapas do Projeto

### 1. Explora√ß√£o Inicial dos Dados
- An√°lise descritiva
- Identifica√ß√£o de multicolinearidade (ex: matriz de correla√ß√£o, VIF)

### 2. Modelos a Serem Aplicados
- **Regress√£o Linear M√∫ltipla**
  - Antes do tratamento da multicolinearidade
  - Ap√≥s o tratamento (ex: remo√ß√£o de vari√°veis, PCA)
- **Regress√£o Ridge**
- **Regress√£o Lasso**

### 3. Estrat√©gia de Valida√ß√£o
- Divis√£o dos dados em **conjunto de treino e teste**
- Aplica√ß√£o de **valida√ß√£o cruzada** (ex: K-Fold)

### 4. Avalia√ß√£o de Desempenho
- M√©tricas de erro:
  - MAPE (Mean Absolute Percentage Error)
  - MSE (Mean Squared Error)
  - RMSE (Root Mean Squared Error)
  - MAE (Mean Absolute Error)
- Crit√©rios de informa√ß√£o:
  - AIC (Akaike Information Criterion)
  - BIC (Bayesian Information Criterion)

## üß† Ferramentas e Tecnologias Sugeridas
- Linguagem: Python (bibliotecas como `scikit-learn`, `statsmodels`, `pandas`, `numpy`)
- Visualiza√ß√£o: `matplotlib`, `seaborn`

## üìå Resultados Esperados
- Comparativo entre os modelos com base nas m√©tricas e crit√©rios
- Discuss√£o sobre o impacto da multicolinearidade
- Recomenda√ß√£o do modelo mais adequado para os dados analisados
"""

# Manipula√ß√£o de Dados
import pandas as pd
import numpy as np

# Visualiza√ß√£o de Dados
import matplotlib.pyplot as plt
import seaborn as sns

# An√°lise de Regress√£o
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Regres√£o Rigde e Lasso
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error,  mean_absolute_percentage_error

# Valida√ß√£o Cruzada
from sklearn.model_selection import KFold

# Sele√ß√£o de Modelos
from sklearn.model_selection import GridSearchCV

"""# **(1) Leitura e Pr√©-processamente de Dados**"""

# Caminho do arquivo.XLSX
path = "https://github.com/MarioDhiego/Ridge_Regression/blob/main/dados_pinus.xlsx?raw=true" # "C:/Users/user/Documents/DETRAN/REGRESS√ÉO - DIEGO/DADOS.xlsx"

# Leitura do arquivo.XLSX
df = pd.read_excel(path)

# Informa√ß√µes
df.info()

# Primeiras Linhas
df.head()

# √öltimas Linhas
df.tail()

"""# **(2) An√°lise Explorat√≥ria de Dados**"""

# Configura√ß√µes de Figura
fig, axes = plt.subplots(2, 4, figsize=(16, 6), dpi=600)

# Itera√ß√£o de Constru√ß√£o Gr√°fica
for column, ax in zip(df.columns, axes.flatten()):
  # Boxplot
  sns.boxplot(x=column, data=df, ax=ax)

  # Configura√ß√µes de eixos
  ax.set_xlabel(column, fontsize=12, weight="bold")

  # Outras configura√ß√µes
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

# Ajuste do Layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

# Configura√ß√µes de Figura
fig, axes = plt.subplots(2, 4, figsize=(16, 6), dpi=600)

# Itera√ß√£o de Constru√ß√£o Gr√°fica
for column, ax in zip(df.columns, axes.flatten()):
  # Histograma & Estima√ß√£o da Densidade
  sns.histplot(x=column, data=df, edgecolor="white", kde=True, line_kws={"color": "red"}, ax=ax)

  # Configura√ß√µes de eixos
  ax.set_xlabel(column, fontsize=12, weight="bold")
  ax.set_ylabel("Frequ√™ncia", fontsize=12, weight="bold")

  # Outras configura√ß√µes
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

# Ajuste do Layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

# Configura√ß√µes de Figura
fig, axes = plt.subplots(2, 4, figsize=(16, 6), dpi=600)

# Itera√ß√£o de Constru√ß√£o Gr√°fica
for column, ax in zip(df.drop(columns="VOLUME").columns, axes.flatten()):
  # Scatterplot
  sns.scatterplot(x=column, y="VOLUME", data=df, hue="VOLUME", palette="viridis", ax=ax)

  # Configura√ß√µes de eixos
  ax.set_xlabel(column, fontsize=12, weight="bold")
  ax.set_ylabel("VOLUME", fontsize=12, weight="bold")

  # Configura√ß√µes de Legenda
  ax.legend(prop={"size":10}, loc="center left", bbox_to_anchor=(1, 0.5), frameon=False)

  # Outras configura√ß√µes
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

# Remover subplots vazios
fig.delaxes(axes.flat[-1])

# Ajuste do Layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

# Criar Pairplot
sns.pairplot(df, corner=True)

# Exibi√ß√£o da Figura
plt.show()

# Matrix de Correla√ß√£o
corr = df.corr(method="pearson")

# Criar uma m√°scara para a parte triangular superior (mantendo a diagonal principal)
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)

# Configura√ß√µes de Figura
fig, ax = plt.subplots(figsize=(10, 6), dpi=600)

# Mapa de Calor com m√°scara
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", linewidths=1
            , vmin=-1, center=0, vmax=1, ax=ax, mask=mask)

# Ajuste do layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

"""## **(2.2) An√°lise de Correla√ß√£o**

* C√°lculo do Coeficiente de Correla√ß√£o Linear de Pearson ($\widehat{\rho}$).
* Teste $t$ para *Signific√¢ncia* de $\widehat{\rho}$.

    * Hip√≥teses:
      $$
      \begin{cases}
      H_{0}: \text{ N√£o h√° correla√ß√£o significativa, } \rho = 0 \\
      H_{1}: \text{ H√° correla√ß√£o significativa, } \rho \neq 0
      \end{cases}
      $$
        
    * Estat√≠stica de Teste:
      $$
      t = \dfrac{ \widehat{\rho} (n - 2)}{\sqrt{1 - \widehat{\rho}^2}} \quad \sim \quad t_{n - 2}
      $$
"""

from scipy.stats import pearsonr

# N√∫mero de vari√°veis
n_var = df.shape[1]

# Matriz de Resultados
r_matrix = np.zeros(shape=(n_var, n_var))

# Aplicar o Teste t para o Coeficiente Linear de Pearson
for i in range(n_var):
    for j in range(n_var):
        # Sele√ß√£o de Vari√°veis
        var1, var2 = df.columns[i],  df.columns[j]

        # Probabilidade de Signific√¢ncia do Teste
        coef, p_value = pearsonr(df[var1], df[var2])

        # Armazenamento de Dados
        r_matrix[i, j] = p_value

# Formata√ß√£o dos Resultados
r_matrix = pd.DataFrame(r_matrix, columns=df.columns, index=df.columns)

# Criar uma m√°scara para a parte triangular superior (mantendo a diagonal principal)
mask = np.triu(np.ones_like(r_matrix, dtype=bool))

# Configura√ß√µes de Figura
fig, ax = plt.subplots(figsize=(10, 6), dpi=600)

# Mapa de Calor com m√°scara
sns.heatmap(r_matrix, annot=True, cmap="viridis", fmt=".4f", linewidths=1, vmin=0, vmax=1, mask=mask, ax=ax)

# T√≠tulo
ax.set_title("Mapa de Calor da Matriz de P-Valores do Teste $t$ para o Coef de Corr de Pearson", fontsize=16, weight="bold")

# Ajuste do layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

"""## **(2.1) Fator de Infla√ß√£o de Vari√¢ncia (VIF)**

O Fator de Infla√ß√£o de Vari√¢ncia (VIF) √© uma medida estat√≠stica utilizada para avaliar o grau de multicolinearidade em um modelo de regress√£o m√∫ltipla. Multicolinearidade ocorre quando duas ou mais vari√°veis independentes em um modelo est√£o altamente correlacionadas, o que pode levar a estimativas de coeficientes inst√°veis e erros padr√£o inflados. Um VIF alto indica que a vari√°vel preditora est√° fortemente correlacionada com outras vari√°veis independentes no modelo, o que pode afetar a interpreta√ß√£o e a precis√£o do modelo.

* Interpreta√ß√£o dos valores de VIF:

    1. $VIF = 1$: N√£o h√° multicolinearidade. A vari√°vel preditora n√£o est√° correlacionada com outras vari√°veis independentes no modelo.
    2. $VIF \leq 5$: H√° multicolinearidade moderada. A correla√ß√£o entre a vari√°vel e outras vari√°veis independentes √© observada, mas n√£o √© considerada cr√≠tica.
    3. $VIF > 5$: H√° multicolinearidade alta. A correla√ß√£o entre a vari√°vel e outras vari√°veis independentes √© alta, o que pode levar a problemas na interpreta√ß√£o e precis√£o do modelo.
    4. $VIF > 10$: H√° multicolinearidade grave. A correla√ß√£o entre a vari√°vel e outras vari√°veis independentes √© muito alta, o que pode levar a resultados pouco confi√°veis do modelo.
"""

# Matriz Design
X = df.drop(columns="VOLUME")

# Adicionando o Intercpto
X = sm.add_constant(X)

# Calcular o Fator de Infla√ß√£o de Varia√ß√£o (VIF)
VIF = pd.DataFrame(index=pd.Series(X.columns, name="Features"))
VIF["VIF"] = [variance_inflation_factor(X.values, id) for id in range(X.shape[1])]

# Excluindo o Intercpto da Visualiza√ß√£o
VIF.drop(index="const", inplace=True)

# Visualizar
VIF

"""## **(2.3) Decis√£o**"""

# Combinando Informa√ß√µes
decision = pd.DataFrame({"VIF": VIF.values.flatten(), "P>|t|": r_matrix.drop(index="VOLUME")["VOLUME"].values}, index=VIF.index)

# Visualizar
decision.round(2)

"""* **Resultados:**
    1. **√† Vari√°vel `IDADE`:** A vari√°vel `IDADE` apresentou $VIF \approx 1,04$ (n√£o preocupante), por√©m a correla√ß√£o entre a `IDADE` e o `VOLUME` (vari√°vel resposta) foi n√£o significativa ($\alpha = 0.05$). Logo, ser√° convidada a se retirar do modelo.
    2. **√† Vari√°vel `DAP`:** A vari√°vel `DAP` apresentou $VIF \approx 65,63$ (bastante preocupante) e a correla√ß√£o entre a `DAP` e o `VOLUME` (vari√°vel resposta) foi significativa ($\alpha = 0.05$).
    3. **√† Vari√°vel `ALTURA`:** A vari√°vel `ALTURA` apresentou $VIF \approx 1,03$ (n√£o preocupante) e a correla√ß√£o entre a `ALTURA` e o `VOLUME` (vari√°vel resposta) foi significativa ($\alpha = 0.05$).
    4. **√† Vari√°vel `IAF`:** A vari√°vel `IAF` apresentou $VIF \approx 6,65$ (pouco preocupante), por√©m a correla√ß√£o entre a `IAF` e o `VOLUME` (vari√°vel resposta) n√£o foi significativa ($\alpha = 0.05$).
    5. **√† Vari√°vel `DAF`:** A vari√°vel `DAF` apresentou $VIF \approx 1,04$ (pouco preocupante), por√©m a correla√ß√£o entre a `DAF` e o `VOLUME` (vari√°vel resposta) n√£o foi significativa ($\alpha = 0.05$). Logo, ser√° convidada a se retirar do modelo.
    6. **√† Vari√°vel `GAP`:** A vari√°vel `GAP` apresentou $VIF \approx 6,69$ (pouco preocupante), por√©m a correla√ß√£o entre a `GAP` e o `VOLUME` (vari√°vel resposta) n√£o foi significativa ($\alpha = 0.05$).
    7. **√† Vari√°vel `AREA_BASAL`:** A vari√°vel `AREA_BASAL` apresentou $VIF \approx 65,78$ (batsante preocupante), por√©m a correla√ß√£o entre a `AREA_BASAL` e o `VOLUME` (vari√°vel resposta) n√£o foi significativa ($\alpha = 0.05$).

* **Conclus√£o:** Ap√≥s a an√°lise explorat√≥ria chegou-se h√° algumas conclus√µes. Foi provado que as vari√°veis `IDADE`, `IAF`, `DAF` e `GAP` apresentaram correla√ß√£o n√£o significativa, ao n√≠vel de $5\%$. Foi visto que `DAP` e `AREA_BASAL` t√™m alta correla√ß√£o entre si fazendo com que ambas tenham um enorme $VIF$, o que afeta as estimativas obtidas pelo modelo. Por√©m, a literatura (da √°rea) diz que n√£o existe `VOLUME` sem `DAP`, o que desta forma, nos leva a retirar a vari√°vel `AREA_BASAL`.
"""

# Vari√°veis que deixaram de compor o modelo
drop_features = ["IDADE", "IAF", "DAF", "GAP", "AREA_BASAL"]

# Exclus√£o de vari√°veis
df_copy = df.drop(columns=drop_features)

# N√∫mero de vari√°veis
n_var = df_copy.shape[1]

# Matriz de Resultados (p-valor)
r_matrix = np.zeros(shape=(n_var, n_var))

# Aplicar o Teste t para o Coeficiente Linear de Pearson
for i in range(n_var):
    for j in range(n_var):
        # Sele√ß√£o de Vari√°veis
        var1, var2 = df_copy.columns[i],  df_copy.columns[j]

        # Probabilidade de Signific√¢ncia do Teste
        coef, p_value = pearsonr(df_copy[var1], df_copy[var2])

        # Armazenamento de Dados
        r_matrix[i, j] = p_value

# Formata√ß√£o dos Resultados
r_matrix = pd.DataFrame(r_matrix, columns=df_copy.columns, index=df_copy.columns)

# Nova Matriz Design
X = df_copy.drop(columns="VOLUME")

# Adicionando o Intercpto
X = sm.add_constant(X)

# Calcular o Fator de Infla√ß√£o de Varia√ß√£o (VIF)
VIF = pd.DataFrame(index=pd.Series(X.columns, name="Features"))
VIF["VIF"] = [variance_inflation_factor(X.values, id) for id in range(X.shape[1])]

# Excluindo o Intercpto da Visualiza√ß√£o
VIF.drop(index="const", inplace=True)

# Combinando Informa√ß√µes
decision = pd.DataFrame({"VIF": VIF.values.flatten(), "P>|t|": r_matrix.drop(index="VOLUME")["VOLUME"].values}, index=VIF.index)

# Visualizar
decision

# Vari√°vel Resposta
y = df_copy["VOLUME"]

# Definindo algumas vari√°veis
n = len(y)                           # Tamanho da amostra
p = X.drop(columns="const").shape[1] # N√∫meros de Preditores
k = p + 1                            # N√∫mero de Coeficientes

"""# **(3) Modelagem**

## **(3.1) Regress√£o Linear M√∫ltipla**
"""

# An√°lise Individual - Teste t
def individual_analysis(
        model: sm.OLS,
        alpha: float = 0.05,
        binary_decision: bool = False
) -> pd.DataFrame:
    """
    Monta uma Tabela (DataFrame) de An√°lise Individual para os
    Coeficientes da Regress√£o Linear

    Args:
        model (sm.OLS): Modelo ajustado via statsmodels.
        alpha (float, opcional): N√≠vel de Signific√¢ncia. Default √© 0.05.
        binary_decision (bool, opcional): Se True, inclui coluna bin√°ria de signific√¢ncia. Default √© False.

    Returns:
        pd.DataFrame: DataFrame com os resultados da an√°lise individual.
    """
    # Vari√°veis do Modelo
    predictors = model.model.exog_names

    # Obter Resultados
    coefs = model.params                   # Coeficientes
    se = model.bse                         # Erro Padr√£o
    conf_int = model.conf_int(alpha=alpha) # Intervalo de Confian√ßa
    t_values = model.tvalues               # Estat√≠stica t
    p_values = model.pvalues               # P-valor

    # Formato DataFrame
    rls = {"Coef": coefs, "Std Err": se, "t": t_values, "P>|t|": p_values}
    rls = pd.DataFrame(rls, index=predictors)
    rls = pd.concat([rls, conf_int], axis=1)
    rls.columns = ["Coef", "Std Err", "t", "P>|t|", "Lower Bound", "Upper Bound"]

    # Coluna de Signific√¢ncia Bin√°ria
    if binary_decision:
        rls["Significance"] = rls["P>|t|"] < alpha

    # Retornar
    return rls

# An√°lise de Vari√¢ncia - ANOVA (Teste F)
def anova_lm(model: sm.OLS) -> pd.DataFrame:
    """
    Cria uma tabela de ANOVA para um modelo de regress√£o m√∫ltipla.

    Par√¢metros:
    model (sm.OLS): Objeto do modelo ajustado de regress√£o m√∫ltipla.

    Args:
        model (sm.OLS): Modelo ajustado via statsmodels.
    Returns:
        pd.DataFrame: DataFrame com a ANOVA.
    """
    # Obter Graus de Liberdade
    df_reg = model.df_model      # Regress√£o
    df_resid = model.df_resid    # Res√≠duo
    df_total = df_reg + df_resid # Total

    # Soma de Quadrados
    SS_reg = model.ess           # Regress√£o
    SS_resid = model.ssr         # Res√≠duo
    SS_total = SS_reg + SS_resid # Total

    # Quadrado M√©dio
    MS_reg = model.mse_model
    MS_resid = model.mse_resid

    # Estat√≠stica F & P-valor
    f, p_value = model.fvalue, model.f_pvalue

    # Criar DataFrame ANOVA
    ANOVA = pd.DataFrame(
        {
            "Degrees of Freedom": [df_reg, df_resid, df_total],
            "Sum of Squares": [SS_reg, SS_resid, SS_total],
            "Mean Square": [MS_reg, MS_resid, None],
            "F": [f, None, None],
            "P>|F|": [p_value, None, None]
        }, index=pd.Series(["Regression", "Residual", "Total"], name="Source of Variation")
    )

    # Retornar
    return ANOVA

"""### **(3.1.1) Constru√ß√£o e Ajuste do Modelo**"""

# Criea√ß√£o & Ajuste do Modelo
lm_model = sm.OLS(y, X).fit()

"""### **(3.1.2) An√°lise Inividual - Teste $t$**"""

individual_analysis(lm_model, 0.05, True).round(4)

"""### **(3.1.3) An√°lise Conjunta - Teste $F$**"""

anova_lm(lm_model).round(4)

"""### **(3.1.4) An√°lise dos Res√≠duos**"""

print(lm_model.summary2())

from scipy.stats import shapiro, ks_1samp, jarque_bera, norm, probplot
from statsmodels.stats.diagnostic import het_breuschpagan, het_white
from statsmodels.stats.stattools import durbin_watson
from statsmodels.graphics.tsaplots import plot_acf

# Res√≠duos do Modelo
residuals = lm_model.resid

"""#### **(3.1.4.1) Verifica√ß√£o do Pressuposto de Normalidade**

* Hip√≥teses: $$\begin{cases} H_{0}: \text{Os res√≠duos s√£o normalmente distribu√≠dos.} \\ H_{1}: \text{Os res√≠duos n√£o s√£o normalmente distribu√≠dos.}\end{cases}$$
"""

# Teste Shapiro-Wilk
shapiro_test = shapiro(residuals)

# Teste Kolmogorov-Smirnov
#ks_test = ks_1samp(residuals, cdf=norm.cdf, args=(residuals.mean(), residuals.std(ddof=1)))
ks_test = ks_1samp(residuals, norm.cdf, method="auto")

# Teste de Jarque-Bera
jarque_bera_test = jarque_bera(residuals)

# Visualiza√ß√£o dos Resultados
print(
    f"""
    Resultados dos Testes de Normalidade:
    {"--" * 40}
    1. Teste Shapiro-Wilk:
        - Estat√≠stica: {shapiro_test.statistic:.4f}
        - P-valor: {shapiro_test.pvalue:.4f}
    {"--" * 40}
    2. Teste Kolmogorov-Smirnov:
        - Estat√≠stica: {ks_test.statistic:.4f}
        - P-valor: {ks_test.pvalue:.4f}
    {"--" * 40}
    3. Teste Jarque-Bera:
        - Estat√≠stica: {jarque_bera_test.statistic:.4f}
        - P-valor: {jarque_bera_test.pvalue:.4f}
    {"--" * 40}
    """
)

# Configura√ß√µes da Figura
fig, axes = plt.subplots(1, 2, figsize=(8, 4), dpi=600)

# Histograma
sns.histplot(residuals, bins=30, edgecolor="white", kde=True, ax=axes[0])

# QQ Plot
probplot(residuals, dist="norm", plot=axes[1])

# Configura√ß√µes de eixos
axes[0].set_xlabel("Res√≠duos", fontsize=12, weight="bold")
axes[0].set_ylabel("Frequ√™ncia", fontsize=12, weight="bold")
axes[1].set_xlabel("Quantis Te√≥ricos", fontsize=12, weight="bold")
axes[1].set_ylabel("Quantis da Amostra", fontsize=12, weight="bold")

# Configura√ß√µes de t√≠tulos
axes[0].set_title("")
axes[1].set_title("")

# Outras configura√ß√µes
for ax in axes:
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

# Ajuste do layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

"""#### **(3.1.4.2) Verifica√ß√£o do Pressuposto de Homocedasticidade/Homogeneidade**

* Hip√≥teses: $$\begin{cases} H_{0}: \text{Os res√≠duos t√™m vari√¢ncia constante.} \\ H_{1}: \text{Os res√≠duos n√£o t√™m vari√¢ncia constante.}\end{cases}$$
"""

# Realiza o Teste de Breusch-Pagan
test_bp = het_breuschpagan(residuals, X)

# Realiza o Teste de White
test_white = het_white(residuals, X)

# Visualiza√ß√£o dos Resultados
print(
    f"""
    Resultados dos Testes de Homocedasticidade/Homogeneidade:
    {"--" * 40}
    1. Teste Breusch-Pagan:
        - Estat√≠stica: {test_bp[0]:.4f}
        - P-valor: {test_bp[1]:.4f}
    {"--" * 40}
    2. Teste White:
        - Estat√≠stica: {test_white[0]:.4f}
        - P-valor: {test_white[1]:.4f}
    {"--" * 40}
    """
)

# Configura√ß√µes da Figura
fig, ax = plt.subplots( figsize=(6, 4), dpi=600)

# Res√≠duos versus Valores Ajustados
sns.scatterplot(x=lm_model.fittedvalues, y=residuals, ax=ax)

# Configura√ß√µes de eixos
ax.set_xlabel("Valores Ajustados", fontsize=12, weight="bold")
ax.set_ylabel("Res√≠duos", fontsize=12, weight="bold")

# Outras configura√ß√µes
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

# Exibi√ß√£o da Figura
plt.show()

"""#### **(3.1.4.3) Verifica√ß√£o do Pressuposto de Independ√™ncia**

* Hip√≥teses: $$\begin{cases} H_{0}: \text{Os res√≠duos s√£o independentes.} \\ H_{1}: \text{Os res√≠duos n√£o s√£o independentes.}\end{cases}$$
"""

# Realizando o Teste Durbin-Watson
test_dw = durbin_watson(residuals)

# Visualiza√ß√£o dos Resultados
print(f"Teste Durbin-Watson para Autocorrela√ß√£o: {test_dw:.2f}")

# Ordem de Observa√ß√£o
idx = np.arange(1, len(residuals) + 1)

# Configura√ß√µes da Figura
fig, axes = plt.subplots(1, 2, figsize=(10, 4), dpi=600)

# Res√≠duos versus Ordem de Observa√ß√£o
sns.scatterplot(x=idx, y=residuals, ax=axes[0])

# Autocorrela√ß√£o
plot_acf(residuals, lags=50, ax=axes[1])

# Configura√ß√µes de eixos
axes[0].set_xlabel("Ordem de Observa√ß√£o", fontsize=12, weight="bold")
axes[0].set_ylabel("Res√≠duos", fontsize=12, weight="bold")
axes[1].set_xlabel("Lags", fontsize=12, weight="bold")
axes[1].set_ylabel("Autocorrela√ß√£o", fontsize=12, weight="bold")

# Outras configura√ß√µes
for ax in axes:
    ax.set_title("")
    ax.spines["top"].set_visible(False)
    ax.spines["right"].set_visible(False)

# Ajuste do layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

"""#### **(3.1.4.4) Influ√™ncia e Outliers**

* T√©cnicas Gr√°ficas
"""

# Diagonal da Matriz de Proje√ß√£o
h_ii = lm_model.get_influence().summary_frame()["hat_diag"]

# Dist√¢ncia de Cook
d_cook = lm_model.get_influence().summary_frame()["cooks_d"]

# DFFITS
dffits = lm_model.get_influence().summary_frame()["dffits"]

# Diagn√≥sticos
threshold = 2 * k / n                        # de Alavanca
threshold_f = 1                              # Dist√¢ncia de Cook
threshold_dffits = 2 * (k / (n - k)) ** 0.5  # DFFITS

# Formata√ß√£o dos Resultados
influence_df = pd.DataFrame(
    {
      "Leverage": h_ii, "Leverage Info": np.where(h_ii >= threshold, "Influente", "N√£o Influente"),
      "Dist√¢ncia de Cook": d_cook, "Dist√¢ncia de Cook Info": np.where(d_cook >= threshold_f, "Influente", "N√£o Influente"),
      "DFFITS": dffits, "DFFITS Info": np.where(dffits >= threshold_dffits, "Influente", "N√£o Influente"),
      "Observa√ß√£o": np.arange(1, n + 1, dtype=int)
    }
).set_index("Observa√ß√£o")

# Visualizar
influence_df

# Configura√ß√µes da Figura
fig, axes = plt.subplots(3, 1, figsize=(12, 9), dpi=600)

# Reajustando o formato do vetor dos eixos
axes = axes.flatten()

# Leverage (Alavancagem)
sns.scatterplot(x=influence_df.index, y=influence_df["Leverage"],
                size=influence_df["Leverage Info"], size_order=["Influente", "N√£o Influente"],
                ax=axes[0])

# Threshold
axes[0].axhline(threshold, color="red", linestyle="--", label=r"$\dfrac{2k}{n} =$"+f"{threshold:.2f}")

# Anotar a observa√ß√£o influente no plot
for idx, row in influence_df.iterrows():
    if row["Leverage"] >= threshold:
      axes[0].annotate(f"{idx}", (idx, row["Leverage"]), fontsize=10, weight="bold")

# Dist√¢ncia de Cook
sns.scatterplot(x=influence_df.index, y=influence_df["Dist√¢ncia de Cook"],
                size=influence_df["Dist√¢ncia de Cook Info"], size_order=["Influente", "N√£o Influente"],
                ax=axes[1])

# Threshold
axes[1].axhline(threshold_f, color="red", linestyle="--", label=r"$F_{k, n-k, 0.5} \approx$"+f"{threshold_f}")

# Anotar a observa√ß√£o influente no plot
for idx, row in influence_df.iterrows():
    if row["Dist√¢ncia de Cook"] >= threshold_f:
      axes[1].annotate(f"{idx}", (idx, row["Dist√¢ncia de Cook"]), fontsize=10, weight="bold")

# DFFITS
sns.scatterplot(x=influence_df.index, y=influence_df["DFFITS"],
                size=influence_df["DFFITS Info"], size_order=["Influente", "N√£o Influente"],
                ax=axes[2])

# Threshold
axes[2].axhline(threshold_dffits, color="red", linestyle="--", label=r"$2 \sqrt{\dfrac{k}{n - k}} =$"+f"{threshold_dffits:.2f}")

# Anotar a observa√ß√£o influente no plot
for idx, row in influence_df.iterrows():
    if row["DFFITS"] >= threshold_dffits:
      axes[2].annotate(f"{idx}", (idx, row["DFFITS"]), fontsize=10, weight="bold")

# Itera√ß√£o para configura√ß√£o
for ax in axes:
  # Configura√ß√µes de eixos
  ax.set_xlabel(ax.get_xlabel(), fontsize=12, weight="bold")
  ax.set_ylabel(ax.get_ylabel(), fontsize=12, weight="bold")

  # Configura√ß√µes de Legenda
  ax.legend(prop={"size": 12}, loc="center left", bbox_to_anchor=(1, 0.5), frameon=False)

  # Outras configura√ß√µes
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

# Ajuste do layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()

# Res√≠duos Ordin√°rios
# residuals

# Res√≠duos Padronizados (Standartized Residuals)
r_i = lm_model.get_influence().summary_frame()["standard_resid"]

# Res√≠duos Studentizados (Studentized Residuals)
t_i = lm_model.get_influence().summary_frame()["student_resid"]

# Configura√ß√µes da Figura
fig, axes = plt.subplots(1, 2, figsize=(12, 4), dpi=600)

# Identifica√ß√£o de Outiliers
sns.scatterplot(x=np.arange(1, n + 1, dtype=int), y=r_i, ax=axes[0])
axes[0].set_ylabel("Res√≠duos Padronizados", fontsize=12, weight="bold")

# Identifica√ß√£o de Outiliers
sns.scatterplot(x=np.arange(1, n + 1, dtype=int), y=t_i, ax=axes[1])
axes[1].set_ylabel("Res√≠duos Studentizados", fontsize=12, weight="bold")

# Itera√ß√£o para Configura√ß√µes
for ax in axes:
  # Configura√ß√µes de eixos
  ax.set_xlabel("Observa√ß√£o", fontsize=12, weight="bold")
  ax.set_ylim(-4, 4)

  # Linhas de Refer√™ncia
  ax.axhline(3, color="red", linestyle="--", label=r"$3 \times \sigma$")      # 3 x sigma
  ax.axhline(0, color="green", linestyle="--", label=r"$E[\varepsilon] = 0$") # mu
  ax.axhline(-3, color="red", linestyle="--", label=r"$-3 \times \sigma$")    # -3 x sigma

  # Configura√ß√µes de Legenda
  ax.legend(prop={"size": 12}, loc="center left", bbox_to_anchor=(1, 0.5), frameon=False)

  # Outras configura√ß√µes
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

# Ajuste do layout e Exibi√ß√£o da Figura
fig.tight_layout()
plt.show()